{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9cad3b-13b1-46e2-99f3-c8a177437ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078febec-b4ff-4e69-bb4b-b5d6fc67f5cc",
   "metadata": {},
   "source": [
    "# Statement Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef46e7-282d-43d4-8257-099ebcfd464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the folder path for the statement datasets\n",
    "folder_path = \"statement/\"\n",
    "\n",
    "# get the account names\n",
    "account_list = []\n",
    "\n",
    "for name in os.listdir(folder_path):\n",
    "    if os.path.isdir(os.path.join(folder_path, name)):\n",
    "        account_list.append(name)\n",
    "\n",
    "account_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877e26f-7186-42c1-9a1a-ff09747c3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function of cleaning the Discover credit card\n",
    "def get_dis_cc(file_path):\n",
    "    df = pd.read_csv(file_path).rename(columns = {'Trans. Date': 'Date'})\n",
    "    df['Amount'] = -df['Amount']\n",
    "    df[\"date_str\"] = pd.to_datetime(df[\"Date\"], format=\"%m/%d/%Y\").dt.strftime(\"%Y%m%d\")\n",
    "    df[\"row_num\"] =  (df.groupby(\"date_str\").cumcount() + 1).astype(str).str.zfill(3)\n",
    "    df[\"Id\"] = os.path.splitext(os.path.basename(file_path))[0] + '_' + df[\"date_str\"] + df[\"row_num\"]\n",
    "    df = df[['Date', 'Id', 'Description', 'Amount']]\n",
    "    return df\n",
    "\n",
    "\n",
    "# define the function of cleaning the BOA credit card\n",
    "def get_boa_cc(file_path):\n",
    "    df = pd.read_csv(file_path, dtype={\"Id\": str}).rename(columns = {'Posted Date': 'Date', 'Payee': 'Description'})\n",
    "    df[\"date_str\"] = pd.to_datetime(df[\"Date\"], format=\"%m/%d/%Y\").dt.strftime(\"%Y%m%d\")\n",
    "    df[\"row_num\"] =  (df.groupby(\"date_str\").cumcount() + 1).astype(str).str.zfill(3)\n",
    "    df[\"Id\"] = os.path.splitext(os.path.basename(file_path))[0] + '_' + df[\"date_str\"] + df[\"row_num\"]\n",
    "    if 'remove' in df.columns:\n",
    "        df = df[df['remove'].isna()]\n",
    "    df = df[['Date', 'Id', 'Description', 'Amount']]\n",
    "    return df\n",
    "\n",
    "\n",
    "# define the function of cleaning the BOA debit card\n",
    "def get_boa_dc(file_path):\n",
    "    df = pd.read_csv(file_path, skiprows = 6).rename(columns = {'Running Bal.': 'Running_balance'})\n",
    "    df[\"date_str\"] = pd.to_datetime(df[\"Date\"], format=\"%m/%d/%Y\").dt.strftime(\"%Y%m%d\")\n",
    "    df[\"row_num\"] =  (df.groupby(\"date_str\").cumcount() + 1).astype(str).str.zfill(3)\n",
    "    df[\"Id\"] = os.path.splitext(os.path.basename(file_path))[0] + '_' + df[\"date_str\"] + df[\"row_num\"]\n",
    "\n",
    "    df['Amount'] = df['Amount'].apply(lambda x: str(x).replace(\",\", \"\")).astype(float)\n",
    "    df['Running_balance'] = df['Running_balance'].apply(lambda x: str(x).replace(\",\", \"\")).astype(float)\n",
    "    df = df[['Date', 'Id', 'Description', 'Amount', 'Running_balance']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90bec8b-2ef5-4592-a7f3-975f28e8c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function of combining all the datasets to one\n",
    "def get_data(head):\n",
    "    \n",
    "    df_list, folder_path = [], \"statement/\"\n",
    "    df_folder_path = os.path.join(folder_path, head)\n",
    "    csv_files = [f for f in os.listdir(df_folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    if head == 'CC-5257':\n",
    "        used_func = get_dis_cc\n",
    "    elif head[0: 2] == 'CC':\n",
    "        used_func = get_boa_cc\n",
    "    elif head[0: 2] == 'DC' or head[0: 2] == 'SA':\n",
    "        used_func = get_boa_dc\n",
    "\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(df_folder_path, file)\n",
    "        df_list.append(used_func(file_path))\n",
    "\n",
    "            \n",
    "    combined_df = pd.concat(df_list)\n",
    "    combined_df['Card'] = head\n",
    "    combined_df[\"Date\"] = pd.to_datetime(combined_df[\"Date\"], format=\"%m/%d/%Y\")\n",
    "    user_map = {'DC-8540': 'Wei', 'CC-0401': 'Wei', 'CC-5257': 'Leo', 'SA-7913': 'saving', 'CC-4253': 'Leo', 'DC-9084': 'Leo'}\n",
    "    combined_df[\"User\"] = combined_df['Card'].map(user_map)\n",
    "    \n",
    "    combined_df = combined_df.sort_values('Date').reset_index(drop = True)\n",
    "\n",
    "    if (head[0: 2] == 'DC' or head[0: 2] == 'SA') and round(combined_df['Running_balance'][0] + combined_df['Amount'].sum(),2) != combined_df['Running_balance'].iloc[-1]:\n",
    "        raise ValueError(f\"{head} data is not validate.\")\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168effea-38da-47d3-abd1-246157223861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the datasets\n",
    "CC_dis_df, CC_Leo_df, DC_Leo_df = get_data('CC-5257'), get_data('CC-4253'), get_data('DC-9084')\n",
    "CC_Wei_df, DC_Wei_df = get_data('CC-0401'), get_data('DC-8540')\n",
    "SA_df = get_data('SA-7913')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3307498b-270d-403f-b4b4-37445ca267da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the start date of the file reading and get the start balance and end balance\n",
    "start_date = '2025-01-01'\n",
    "\n",
    "start_balance, end_balance = 0, 0\n",
    "\n",
    "for DC in [DC_Leo_df, DC_Wei_df, SA_df]: \n",
    "    start_balance += DC[DC['Date'] < start_date]['Running_balance'].iloc[-1]\n",
    "\n",
    "for DC in [DC_Leo_df, DC_Wei_df, SA_df]: \n",
    "    end_balance += DC['Running_balance'].iloc[-1]\n",
    "\n",
    "# use only the data after the start date\n",
    "DC_Leo_df = DC_Leo_df[DC_Leo_df['Date'] >= start_date]\n",
    "DC_Wei_df = DC_Wei_df[DC_Wei_df['Date'] >= start_date]\n",
    "SA_df = SA_df[SA_df['Date'] >= start_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba99040-c7b9-46dc-8870-2c81a4e9c563",
   "metadata": {},
   "source": [
    "# Transaction Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0823adb8-6e37-4111-b82f-22212d298807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the description-type matching map\n",
    "description_map = {\n",
    "    'GEICO': 'Auto: Insurance',\n",
    "    'AAA CA MEMBERSHIP': 'Auto: Insurance',\n",
    "    'FARMERS INS': 'Auto: Insurance',\n",
    "    'CHEVRON': 'Auto: Gas',\n",
    "    'MISSION FUEL': 'Auto: Gas',\n",
    "    'COSTCO GAS': 'Auto: Gas',\n",
    "    'CONSERV FUEL': 'Auto: Gas',\n",
    "    'ARCO': 'Auto: Gas',\n",
    "    'NEW CENTURY MAZDA': 'Auto: Maintainence',\n",
    "    'BELLAGIO EXPRESS': 'Auto: Car Wash',\n",
    "    'DMV': 'Auto: DMV fee',\n",
    "    'PARKING': 'Auto: Parking',\n",
    "    'TOLL ROADS': 'Auto: Toll',\n",
    "    \n",
    "    'GOOD FORTUNE SUPERMARKET': 'Grocery: GFM',\n",
    "    'GF MARKET': 'Grocery: GFM',\n",
    "    '99 RANCH': 'Grocery: 99 Ranch',\n",
    "    '7-ELEVEN': 'Grocery: 7-ELEVEN',\n",
    "    'TARGET': 'Grocery: Target',\n",
    "    'CVS/PHARMACY': 'Grocery: CVS',\n",
    "    'COSTCO WHSE': 'Grocery: Costco',\n",
    "    'COSTCO *ANNUAL RENEWAL': 'Grocery: Costco',\n",
    "    'H MART': 'Grocery: HMart',\n",
    "    'WHOLEFDS': 'Grocery: Whole Foods',\n",
    "    'LITTLE PEACH MEAT': 'Grocery: Meat Shop',\n",
    "    'VONS': 'Grocery: others',\n",
    "    'GINSENG': 'Grocery: others',\n",
    "    'DAISO': 'Grocery: others',\n",
    "    'HOME DEPOT': 'Grocery: others',\n",
    "    'SAN GABRIEL SPRSTR': 'Grocery: others',\n",
    "\n",
    "    'LinkedInPre': 'Study: LinkedIn',\n",
    "    'UDEMY': 'Study: Udemy',\n",
    "    'OPENAI': 'Study: ChatGPT',\n",
    "    'GITHUB': 'Study: GitHub',\n",
    "    'Google': 'Study: Google',\n",
    "    'ADOBE': 'Study: Adobe',\n",
    "    'CLAUDE.AI SUBSCRIPTION': 'Study: Claude AI',\n",
    "    'WWW.FREEPIK.CDE': 'Study: Freepik',\n",
    "    'WWW.GLOS.AC.UK': 'Study: Prize Application',\n",
    "    'PAYPAL': 'Study: Prize Application',\n",
    "    'WWW.AIGANY.ORNY': 'Study: Prize Application',\n",
    "    'DEEPL* SUB': 'Study: DeepL',\n",
    "    'DALLAS BAPTIST UNIVERSIT': 'Study: Dallas Baptist',\n",
    "\n",
    "    'LYFT': 'Logistic: Lyft',\n",
    "    'The UPS Store': 'Logistic: UPS/USPS/Fedex',\n",
    "    'USPS': 'Logistic: UPS/USPS/Fedex',\n",
    "    'FEDEX': 'Logistic: UPS/USPS/Fedex',\n",
    "\n",
    "    'CITY OF ARCADIA': 'Utility: Water',\n",
    "    'Spectrum': 'Utility: Spectrum',\n",
    "    'SO CAL EDISON': 'Utility: Edison',\n",
    "    'SO CAL GAS': 'Utility: SoCal Gas',\n",
    "    'SoCalGas': 'Utility: SoCal Gas',\n",
    "    'LA Co TTC Paymnt': 'Utility: Property Tax',\n",
    "    'TMOBILE': 'Utility: T-Mobile',\n",
    "    'Zelle payment to LZ COMFORT HOME': 'Utility: others',\n",
    "    \n",
    "    'Chun La Hao': 'Restaurant: Hotpot',\n",
    "    'HAIDILAO': 'Restaurant: Hotpot',\n",
    "    'CHI HUO': 'Restaurant: Hotpot',\n",
    "    '101 POT': 'Restaurant: Hotpot',\n",
    "    'ERWA COLD POT': 'Restaurant: Sichuan Dish',\n",
    "    'KUAN ZHAI ALLEY': 'Restaurant: Sichuan Dish',\n",
    "    'SICHUAN IMPRESSION': 'Restaurant: Sichuan Dish',\n",
    "    'WANG LA YA INC': 'Restaurant: Sichuan Dish',\n",
    "    'SHANGHAILANDER': 'Restaurant: Shanghai Dish',\n",
    "    'SINBALA': 'Restaurant: Taiwan Dish',\n",
    "    'IN-N-OUT': 'Restaurant: Fast Food',\n",
    "    'HABIT': 'Restaurant: Fast Food',\n",
    "    'RAISING CANES': 'Restaurant: Fast Food',\n",
    "    'POPEYES': 'Restaurant: Fast Food',\n",
    "    'TOFU HOUSE': 'Restaurant: Korean Dish',\n",
    "    'SUSHI': 'Restaurant: Japanese Dish',\n",
    "    'CURRY FLURRY': 'Restaurant: Japanese Dish',\n",
    "    'RAMEN': 'Restaurant: Japanese Dish',\n",
    "    'RUEN PAIR THAI RESTAURAN': 'Restaurant: Thai Dish', \n",
    "    'LADY M': 'Restaurant: Desert',\n",
    "    '85C': 'Restaurant: Desert',\n",
    "    'VANILLA BAKE': 'Restaurant: Desert',\n",
    "    'SUNRIGHT': 'Restaurant: Desert',\n",
    "    'YOGURTLAND': 'Restaurant: Desert',\n",
    "    'MELOMELO': 'Restaurant: Desert',\n",
    "    'AUNTIE ANNES': 'Restaurant: Desert',\n",
    "    'STARBUCKS': 'Restaurant: Desert',\n",
    "    'GELATO': 'Restaurant: Desert',\n",
    "    'PRESSED': 'Restaurant: Desert',\n",
    "    'PATISSERIE BLUEJAY': 'Restaurant: Desert',\n",
    "    'SQ *DOSE': 'Restaurant: Desert',\n",
    "    'MARU COFFEE': 'Restaurant: Desert',\n",
    "\n",
    "    'DD *DOORDASH': 'Online: DoorDash',\n",
    "    'AMAZON': 'Online: Amazon',\n",
    "    'WEEE': 'Online: Weee',\n",
    "    'UBER *EATS': 'Online: Uber Eats',\n",
    "    'YAMIBUY': 'Online: Yami',\n",
    "    'HUNGRYPANDA': 'Online: Hungry Panda',\n",
    "    'WWW.PETFIESTACO': 'Online: others', \n",
    "    'COS WEB': 'Online: others',\n",
    "    \n",
    "    'APPLE': 'Other: Apple',\n",
    "    'HOSPITAL': 'Other: Healthcare',\n",
    "    'ROSE WOMENS HEALTH': 'Other: Healthcare',\n",
    "    'QUEST DIAGNOSTICS': 'Other: Healthcare',\n",
    "    'PRIMROSE PSYCHIATRY': 'Other: Healthcare',\n",
    "    'RADIANT IMAGING': 'Other: Healthcare',\n",
    "    'AMERICAN PEDIATRICS': 'Other: Healthcare',\n",
    "    'CA DEPT OF PUBLIC HEALTH': 'Other: Healthcare',\n",
    "    'VCN*LOSANGELESCODPH': 'Other: Baby related',\n",
    "    'CA SOS BPD LOS ANGELES': 'Other: Baby related',\n",
    "    'BELLA BABY PHOTOGRAPHY': 'Other: Baby related',\n",
    "    'Zelle payment to SHUHUI QIAN': 'Other: Baby related',\n",
    "    'BKOFAMERICA MOBILE': 'Other: Mobile Check',\n",
    "    'Wire Transfer Fee': 'Other: Bank fee',\n",
    "    'LATE FEE': 'Other: Bank fee',\n",
    "    'FOREIGN TRANSACTION FEE': 'Other: Bank fee',\n",
    "    'INTEREST CHARGED': 'Other: Bank fee',\n",
    "    'OVERDRAFT ITEM FEE': 'Other: Bank fee',\n",
    "\n",
    "    'Online payment': 'CC Payback: Money',\n",
    "    'Online Banking payment to CRD 4253': 'CC Payback: Money',\n",
    "    'Online Banking payment to CRD 0401': 'CC Payback: Money',\n",
    "    'DISCOVER DES': 'CC Payback: Discover',\n",
    "    'INTERNET PAYMENT': 'CC Payback: Discover',\n",
    "    'Interest Earned': 'Investment: Interest',\n",
    "    'FID BKG SVC LLC': 'Investment: Fidelity',\n",
    "    'CASH REWARDS STATEMENT CREDIT': 'Investment: CC Credit',\n",
    "    'CASHREWARD': 'Investment: CC Credit',\n",
    "    'CASHBACK BONUS REDEMPTION': 'Investment: CC Credit',\n",
    "\n",
    "    'DES:PAYROLL ID:XXXXX716960': 'Income: Luminys Payroll',\n",
    "    'C185529 LUMINYS': 'Income: Luminys Payroll',\n",
    "    'Money Network DES:': 'Income: Luminys Payroll',\n",
    "    'LUMINYS SYSTEMS DES:PAYMENT': 'Income: Luminys Freelance',\n",
    "    'ISSI INC. DES:PAYROLL': 'Income: ISSI',\n",
    "    'CERTIFY- LUMINYS': 'Income: Reimbursement',\n",
    "    'Zelle payment to QU WU': 'Income: Reimbursement',\n",
    "    \n",
    "    'IRS': 'Tax: IRS',\n",
    "    'FRANCHISE TAX BD DES': 'Tax: CA',\n",
    "    'FRANCHISE TAX BO DES': 'Tax: CA',\n",
    "    \"Zelle payment to ROGER'S TAX SERVICES LLC\": 'Tax: Roger Service',\n",
    "\n",
    "    'Zelle payment from CHENWEI XU': 'Internal: from Chenwei to Leo',\n",
    "    'Zelle payment to CHENWEI XU': 'Internal: from Leo to Chenwei',\n",
    "    'Zelle payment from NINGCHUAN PENG': 'Internal: from Leo to Chenwei',\n",
    "    'Zelle payment to NINGCHUAN PENG': 'Internal: from Chenwei to Leo',\n",
    "    'Online Banking transfer from SAV 7913': 'Internal: from SA to Leo/Chenwei',\n",
    "    'Online Banking transfer to SAV 7913': 'Internal: from Leo/Chenwei to SA',\n",
    "    'Online Banking transfer from CHK 9084': 'Internal: from Leo to SA',\n",
    "    'Online Banking transfer to CHK 9084': 'Internal: from SA to Leo',\n",
    "    'Online Banking transfer from CHK 8540': 'Internal: from Chenwei to SA',\n",
    "    'Online Banking transfer to CHK 8540': 'Internal: from SA to Chenwei',\n",
    "    'WIRE TYPE': 'Internal: Wire',\n",
    "    'Zelle payment to THE CHURCH OF GOD': 'Internal: Church',\n",
    "\n",
    "\n",
    "    'Zelle payment to YUKAI GAO': 'Rent: CTHD',\n",
    "    'Zelle payment from YUKAI GAO': 'Rent: CTHD',\n",
    "    'Zelle payment to XUE SHIMING': 'Rent: Avlon',\n",
    "    'Zelle payment to QINGMING ZENG': 'Rent: LA Fire Villa',\n",
    "    'Zelle payment from GUOYUAN WU': 'Rent: LA Fire Villa',\n",
    "\n",
    "    'AMC': 'Entertainment: AMC',\n",
    "    'LA ARBORETUM': 'Entertainment: Arboretum'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e840d-1efa-4af3-a841-3f47ec800def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the description to the type\n",
    "for keyword, mapped_value in description_map.items():\n",
    "    for df in [CC_dis_df, CC_Leo_df, DC_Leo_df, CC_Wei_df, DC_Wei_df, SA_df]:\n",
    "        df.loc[df[\"Description\"].str.contains(keyword, case=False, regex=False, na=False), \"Type\"] = mapped_value\n",
    "\n",
    "# match the special type\n",
    "DC_Leo_df.loc[DC_Leo_df['Type'] == 'Internal: from SA to Leo/Chenwei', 'Type'] = 'Internal: from SA to Leo'\n",
    "DC_Leo_df.loc[DC_Leo_df['Type'] == 'Internal: from Leo/Chenwei to SA', 'Type'] = 'Internal: from Leo to SA'\n",
    "\n",
    "DC_Wei_df.loc[DC_Wei_df['Type'] == 'Internal: from SA to Leo/Chenwei', 'Type'] = 'Internal: from SA to Chenwei'\n",
    "DC_Wei_df.loc[DC_Wei_df['Type'] == 'Internal: from Leo/Chenwei to SA', 'Type'] = 'Internal: from Chenwei to SA'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e07664-fe66-4314-80bf-f52e480d4857",
   "metadata": {},
   "source": [
    "# Credit Card Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e17df3-1598-4498-a8f7-83c4b7669eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function of credit card data cleaning to make sure the expense amount is the same as payback amount\n",
    "def CC_get(cc_dateset):\n",
    "    if 'CC Payback: Money' in set(cc_dateset['Type']):\n",
    "        cc_max_i = max(cc_dateset[cc_dateset['Type'] == 'CC Payback: Money'].index)\n",
    "    else:\n",
    "        cc_max_i = max(cc_dateset[cc_dateset['Type'] == 'CC Payback: Discover'].index)\n",
    "\n",
    "    i = 0\n",
    "    while round(cc_dateset.loc[i: cc_max_i]['Amount'].sum(), 2) != 0:\n",
    "        i += 1\n",
    "\n",
    "    if cc_dateset.loc[i: cc_max_i].shape[0] == 0:\n",
    "        raise ValueError(f\"The data is not validate.\")\n",
    "    else:\n",
    "        return cc_dateset.loc[i: cc_max_i]\n",
    "\n",
    "# clean the credit card dataset\n",
    "combined_data_list = [DC_Leo_df, CC_get(CC_Leo_df), CC_get(CC_dis_df), DC_Wei_df, CC_get(CC_Wei_df), SA_df]\n",
    "\n",
    "# combine all the datasets to one dataset\n",
    "combined_df = pd.concat(combined_data_list).sort_values('Date').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eda1e8-da73-4761-9a90-0003c83ccf80",
   "metadata": {},
   "source": [
    "# Transaction Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e7721a-3ba1-4289-8188-17fcf74f6c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the columns of the dataset and add new columns\n",
    "combined_df[\"General_Type\"] = combined_df[\"Type\"].str.split(\":\", n=1).str[0]\n",
    "combined_df.loc[combined_df['Type'].isna(), 'Type'] = None\n",
    "combined_df.loc[combined_df['General_Type'].isna(), 'General_Type'] = None\n",
    "\n",
    "# uremove unnecessary rows and columns\n",
    "for remove_item in combined_df['Type'].unique():\n",
    "    if round(combined_df[combined_df['Type'] == remove_item]['Amount'].sum(), 2) == 0 and remove_item != None:\n",
    "        print('remove: ', remove_item)\n",
    "        combined_df = combined_df[combined_df['Type'] != remove_item]\n",
    "combined_df = combined_df[(combined_df['Amount'].notna()) & (combined_df['Amount'] != 0)]\n",
    "\n",
    "combined_df = combined_df[['Date', 'Id', 'Description', 'Amount', 'Card', 'User', 'Type', 'General_Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff9358-9d4b-4f09-89ac-e0bf4cee510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the start amount + changing amount = end amount\n",
    "if round(start_balance + combined_df['Amount'].sum(),2) == end_balance:\n",
    "    print('Starting Balance:', start_balance)\n",
    "    print('Ending Balance:', end_balance)\n",
    "    print('Changing Amount:', round(combined_df['Amount'].sum(),2))\n",
    "else:\n",
    "    raise ValueError(\"Changing Amount is not right.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2b6a0-a88f-4029-bd86-cd7deb37fe93",
   "metadata": {},
   "source": [
    "# Mapping Old File Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e54e0-ada4-4133-b1c8-ef9f0b7ce90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defin the function of mapping the old excel file information to this new old\n",
    "def sub_old_data(new_df, old_file_name):\n",
    "    old_df = pd.read_excel(old_file_name, sheet_name = 'Sheet1')[['Id', 'Type', 'General_Type']]\n",
    "\n",
    "    comb_df = pd.merge(new_df, old_df, on = \"Id\", how = \"left\", suffixes = (\"\", \"_old\"))\n",
    "    \n",
    "    comb_df.loc[(comb_df['Type'] != comb_df['Type_old']) & \n",
    "                (comb_df['Type_old'].notna()), 'Type'] = comb_df.loc[(comb_df['Type'] != comb_df['Type_old']) & \n",
    "                (comb_df['Type_old'].notna()), 'Type_old']\n",
    "\n",
    "    comb_df.loc[(comb_df['General_Type'] != comb_df['General_Type_old']) & \n",
    "                (comb_df['General_Type_old'].notna()), 'General_Type'] = comb_df.loc[(comb_df['General_Type'] != comb_df['General_Type_old']) & \n",
    "                (comb_df['General_Type_old'].notna()), 'General_Type_old']\n",
    "\n",
    "    return comb_df[['Date', 'Id', 'Description', 'Amount', 'Card', 'User', 'Type', 'General_Type']]\n",
    "\n",
    "# map the old excel file information to this new old\n",
    "old_file_name = 'combined_20250821211148.xlsx'\n",
    "combined_df_new = sub_old_data(combined_df, old_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ddbf6-5732-40cf-8eb8-3dc343c62a14",
   "metadata": {},
   "source": [
    "# Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d751b34-4722-43b6-b166-e70a082f0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the file to EXCEL\n",
    "combined_df_new.to_excel(f\"combined_{datetime.now().strftime('%Y%m%d%H%M%S')}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d85583-4ab3-43f4-9ec5-c7dc2b529a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
