{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9cad3b-13b1-46e2-99f3-c8a177437ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os, dotenv\n",
    "\n",
    "from utils.google_api_utils import gsheet_upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078febec-b4ff-4e69-bb4b-b5d6fc67f5cc",
   "metadata": {},
   "source": [
    "# Statement Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef46e7-282d-43d4-8257-099ebcfd464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the folder path for the statement datasets\n",
    "folder_path = \"../personal_envs/household-cashflow-analyzer/private/\"\n",
    "\n",
    "# get the account names\n",
    "account_list = []\n",
    "\n",
    "for name in os.listdir(folder_path):\n",
    "    if os.path.isdir(os.path.join(folder_path, name)):\n",
    "        account_list.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877e26f-7186-42c1-9a1a-ff09747c3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function of cleaning the Discover credit card\n",
    "def get_dis_cc(file_path):\n",
    "    df = pd.read_csv(file_path).rename(columns = {'Trans. Date': 'Date'})\n",
    "    df['Amount'] = -df['Amount']\n",
    "    df[\"date_str\"] = pd.to_datetime(df[\"Date\"], format=\"%m/%d/%Y\").dt.strftime(\"%Y%m%d\")\n",
    "    df[\"row_num\"] =  (df.groupby(\"date_str\").cumcount() + 1).astype(str).str.zfill(3)\n",
    "    df[\"Id\"] = os.path.splitext(os.path.basename(file_path))[0][0: 7] + '_' + df[\"date_str\"] + df[\"row_num\"]\n",
    "    df = df[['Date', 'Id', 'Description', 'Amount']]\n",
    "    return df\n",
    "\n",
    "\n",
    "# define the function of cleaning the BOA credit card\n",
    "def get_boa_cc(file_path):\n",
    "    df = pd.read_csv(file_path, dtype={\"Id\": str}).rename(columns = {'Posted Date': 'Date', 'Payee': 'Description'})\n",
    "    df[\"date_str\"] = pd.to_datetime(df[\"Date\"], format=\"%m/%d/%Y\").dt.strftime(\"%Y%m%d\")\n",
    "    df[\"row_num\"] =  (df.groupby(\"date_str\").cumcount() + 1).astype(str).str.zfill(3)\n",
    "    df[\"Id\"] = os.path.splitext(os.path.basename(file_path))[0][0: 7] + '_' + df[\"date_str\"] + df[\"row_num\"]\n",
    "    df = df[['Date', 'Id', 'Description', 'Amount']]\n",
    "    return df\n",
    "\n",
    "\n",
    "# define the function of cleaning the BOA debit card\n",
    "def get_boa_dc(file_path):\n",
    "    df = pd.read_csv(file_path, skiprows = 6).rename(columns = {'Running Bal.': 'Running_balance'})\n",
    "    df[\"date_str\"] = pd.to_datetime(df[\"Date\"], format=\"%m/%d/%Y\").dt.strftime(\"%Y%m%d\")\n",
    "    df[\"row_num\"] =  (df.groupby(\"date_str\").cumcount() + 1).astype(str).str.zfill(3)\n",
    "    df[\"Id\"] = os.path.splitext(os.path.basename(file_path))[0][0: 7] + '_' + df[\"date_str\"] + df[\"row_num\"]\n",
    "\n",
    "    df['Amount'] = df['Amount'].apply(lambda x: str(x).replace(\",\", \"\")).astype(float)\n",
    "    df = df[df['Amount'].notna()]\n",
    "    df = df[['Date', 'Id', 'Description', 'Amount']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90bec8b-2ef5-4592-a7f3-975f28e8c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function of combining all the datasets to one\n",
    "def get_data(head, folder_path):\n",
    "    \n",
    "    df_list, folder_path = [], folder_path\n",
    "    df_folder_path = os.path.join(folder_path, head)\n",
    "    csv_files = [f for f in os.listdir(df_folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    if head == 'CC-5257':\n",
    "        used_func = get_dis_cc\n",
    "    elif head[0: 2] == 'CC':\n",
    "        used_func = get_boa_cc\n",
    "    elif head[0: 2] == 'DC' or head[0: 2] == 'SA':\n",
    "        used_func = get_boa_dc\n",
    "\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(df_folder_path, file)\n",
    "        df_list.append(used_func(file_path))\n",
    "            \n",
    "    combined_df = pd.concat(df_list)\n",
    "    combined_df['Card'] = head\n",
    "    combined_df[\"Date\"] = pd.to_datetime(combined_df[\"Date\"], format=\"%m/%d/%Y\").dt.date\n",
    "    \n",
    "    user_map = {'DC-8540': 'User2', 'CC-0401': 'User2', 'CC-5257': 'User1', 'SA-7913': 'Savings', 'CC-4253': 'User1', 'DC-9084': 'User1'}\n",
    "    combined_df[\"User\"] = combined_df['Card'].map(user_map)\n",
    "    \n",
    "    combined_df = combined_df.sort_values('Date').reset_index(drop = True)\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf5cca-356f-4102-a154-56512a35f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the start date of the file reading\n",
    "start_date = '2025-01-01'\n",
    "\n",
    "# read the datasets\n",
    "account_df_dict = {}\n",
    "\n",
    "for account in account_list:\n",
    "    df = get_data(account, folder_path)\n",
    "    df = df[df['Date'] >= datetime.strptime(start_date, \"%Y-%m-%d\").date()]\n",
    "    account_df_dict[account] = df\n",
    "    \n",
    "# combine all the datasets to one dataset\n",
    "combined_df = pd.concat([account_df_dict[account] for account in account_list]).sort_values('Date').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba99040-c7b9-46dc-8870-2c81a4e9c563",
   "metadata": {},
   "source": [
    "# Transaction Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0823adb8-6e37-4111-b82f-22212d298807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the description-type matching map\n",
    "description_map = {\n",
    "    'GEICO': 'Auto: Insurance',\n",
    "    'AAA CA MEMBERSHIP': 'Auto: Insurance',\n",
    "    'FARMERS INS': 'Auto: Insurance',\n",
    "    'CHEVRON': 'Auto: Gas',\n",
    "    'MISSION FUEL': 'Auto: Gas',\n",
    "    'COSTCO GAS': 'Auto: Gas',\n",
    "    'CONSERV FUEL': 'Auto: Gas',\n",
    "    'ARCO': 'Auto: Gas',\n",
    "    'NEW CENTURY MAZDA': 'Auto: Maintainence/DMV',\n",
    "    'DMV': 'Auto: Maintainence/DMV',\n",
    "    'BELLAGIO EXPRESS': 'Auto: Wash/Parking/Toll',\n",
    "    'PARKING': 'Auto: Wash/Parking/Toll',\n",
    "    'TOLL ROADS': 'Auto: Wash/Parking/Toll',\n",
    "    \n",
    "    'GOOD FORTUNE SUPERMARKET': 'Grocery: GFM',\n",
    "    'GF MARKET': 'Grocery: GFM',\n",
    "    '99 RANCH': 'Grocery: 99 Ranch',\n",
    "    '7-ELEVEN': 'Grocery: others',\n",
    "    'TARGET': 'Grocery: Target',\n",
    "    'CVS/PHARMACY': 'Grocery: CVS',\n",
    "    'COSTCO WHSE': 'Grocery: Costco',\n",
    "    'COSTCO *ANNUAL RENEWAL': 'Grocery: Costco',\n",
    "    'H MART': 'Grocery: HMart',\n",
    "    'WHOLEFDS': 'Grocery: Whole Foods',\n",
    "    'LITTLE PEACH MEAT': 'Grocery: others',\n",
    "    'VONS': 'Grocery: others',\n",
    "    'GINSENG': 'Grocery: others',\n",
    "    'DAISO': 'Grocery: others',\n",
    "    'HOME DEPOT': 'Grocery: others',\n",
    "    'SAN GABRIEL SPRSTR': 'Grocery: others',\n",
    "\n",
    "    'OPENAI': 'Study: ChatGPT',\n",
    "    'LinkedInPre': 'Study: LinkedIn',\n",
    "    'UDEMY': 'Study: Coding related',\n",
    "    'GITHUB': 'Study: Coding related',\n",
    "    'Google': 'Study: Design related',\n",
    "    'ADOBE': 'Study: Design related',\n",
    "    'Motion Array': 'Study: Design related',\n",
    "    'WWW.FREEPIK.CDE': 'Study: Design related',\n",
    "    'WWW.GLOS.AC.UK': 'Study: Prize Application',\n",
    "    'PAYPAL': 'Study: Prize Application',\n",
    "    'WWW.AIGANY.ORNY': 'Study: Prize Application',\n",
    "    'DALLAS BAPTIST UNIVERSIT': 'Study: others',\n",
    "    'CLAUDE.AI SUBSCRIPTION': 'Study: others',\n",
    "    'DEEPL* SUB': 'Study: others',\n",
    "    \n",
    "    'LYFT': 'Logistic: Lyft/Uber/Transportation',\n",
    "    'The UPS Store': 'Logistic: UPS/USPS/Fedex',\n",
    "    'USPS': 'Logistic: UPS/USPS/Fedex',\n",
    "    'FEDEX': 'Logistic: UPS/USPS/Fedex',\n",
    "    'VCN*LOSANGELESCODPH': 'Logistic: Visa',\n",
    "    'CA SOS BPD LOS ANGELES': 'Logistic: Visa',\n",
    "    \n",
    "    'CITY OF ARCADIA': 'Utility: Water',\n",
    "    'Spectrum': 'Utility: Spectrum',\n",
    "    'SO CAL EDISON': 'Utility: Edison',\n",
    "    'SO CAL GAS': 'Utility: SoCal Gas',\n",
    "    'SoCalGas': 'Utility: SoCal Gas',\n",
    "    'LA Co TTC Paymnt': 'Utility: Property Tax',\n",
    "    'TMOBILE': 'Utility: T-Mobile',\n",
    "    'Zelle payment to LZ COMFORT HOME': 'Utility: others',\n",
    "    \n",
    "    'Chun La Hao': 'Restaurant: Hotpot',\n",
    "    'HAIDILAO': 'Restaurant: Hotpot',\n",
    "    'CHI HUO': 'Restaurant: Hotpot',\n",
    "    '101 POT': 'Restaurant: Hotpot',\n",
    "    'ERWA COLD POT': 'Restaurant: Sichuan Dish',\n",
    "    'KUAN ZHAI ALLEY': 'Restaurant: Sichuan Dish',\n",
    "    'SICHUAN IMPRESSION': 'Restaurant: Sichuan Dish',\n",
    "    'WANG LA YA INC': 'Restaurant: Sichuan Dish',\n",
    "    'SHANGHAILANDER': 'Restaurant: Shanghai Dish',\n",
    "    'SINBALA': 'Restaurant: Taiwan Dish',\n",
    "    'IN-N-OUT': 'Restaurant: Fast Food',\n",
    "    'HABIT': 'Restaurant: Fast Food',\n",
    "    'RAISING CANES': 'Restaurant: Fast Food',\n",
    "    'POPEYES': 'Restaurant: Fast Food',\n",
    "    'TOFU HOUSE': 'Restaurant: Korean Dish',\n",
    "    'SUSHI': 'Restaurant: Japanese Dish',\n",
    "    'CURRY FLURRY': 'Restaurant: Japanese Dish',\n",
    "    'RAMEN': 'Restaurant: Japanese Dish',\n",
    "    'THAI RESTAURAN': 'Restaurant: Thai Dish', \n",
    "    'LADY M': 'Restaurant: Dessert',\n",
    "    'SUNRIGHT': 'Restaurant: Dessert',\n",
    "    'YOGURTLAND': 'Restaurant: Dessert',\n",
    "    'MELOMELO': 'Restaurant: Dessert',\n",
    "    'AUNTIE ANNES': 'Restaurant: Dessert',\n",
    "    'GELATO': 'Restaurant: Dessert',\n",
    "    'PATISSERIE BLUEJAY': 'Restaurant: Dessert',\n",
    "    'PRESSED': 'Restaurant: Dessert',\n",
    "    '85C': 'Restaurant: Bakery & Coffee',\n",
    "    'VANILLA BAKE': 'Restaurant: Bakery & Coffee',\n",
    "    'STARBUCKS': 'Restaurant: Bakery & Coffee',\n",
    "    'SQ *DOSE': 'Restaurant: Bakery & Coffee',\n",
    "    'MARU COFFEE': 'Restaurant: Bakery & Coffee',\n",
    "\n",
    "    'DD *DOORDASH': 'Online: DoorDash',\n",
    "    'AMAZON': 'Online: Amazon',\n",
    "    'WEEE': 'Online: Weee',\n",
    "    'UBER *EATS': 'Online: Uber Eats',\n",
    "    'YAMIBUY': 'Online: Yami',\n",
    "    'HUNGRYPANDA': 'Online: Hungry Panda',\n",
    "    'WWW.PETFIESTACO': 'Online: others', \n",
    "    'COS WEB': 'Online: others',\n",
    "    \n",
    "    'Zelle payment to SHUHUI QIAN': 'Healthcare: Baby',\n",
    "    'METHODIST HOSPITAL': 'Healthcare: Baby',\n",
    "    'RADIANT IMAGING': 'Healthcare: Baby',\n",
    "    'AMERICAN PEDIATRICS': 'Healthcare: Baby',\n",
    "    'QUEST DIAGNOSTICS': 'Healthcare: Baby',\n",
    "    'CA DEPT OF PUBLIC HEALTH': 'Healthcare: Baby',\n",
    "    'PRIMROSE PSYCHIATRY': 'Healthcare: User1',\n",
    "    'ROSE WOMENS HEALTH': 'Healthcare: User2',\n",
    "\n",
    "    'APPLE': 'Other: Apple',\n",
    "    'BKOFAMERICA MOBILE': 'Other: Mobile Check',\n",
    "    'Wire Transfer Fee': 'Other: Bank fee',\n",
    "    'LATE FEE': 'Other: Bank fee',\n",
    "    'FOREIGN TRANSACTION FEE': 'Other: Bank fee',\n",
    "    'INTEREST CHARGED': 'Other: Bank fee',\n",
    "    'OVERDRAFT ITEM FEE': 'Other: Bank fee',\n",
    "    'Zelle payment to THE CHURCH OF GOD': 'Other: Church',\n",
    "    \n",
    "    'Online payment': 'CC Payback: BOA',\n",
    "    'Online Banking payment to CRD 4253': 'CC Payback: BOA',\n",
    "    'Online Banking payment to CRD 0401': 'CC Payback: BOA',\n",
    "    'DISCOVER DES': 'CC Payback: Discover',\n",
    "    'INTERNET PAYMENT': 'CC Payback: Discover',\n",
    "    \n",
    "    'FID BKG SVC LLC': 'Omit: Fidelity',\n",
    "    'WIRE TYPE': 'Omit: Wire',\n",
    "    \n",
    "    'DES:PAYROLL ID:XXXXX716960': 'Income: Company1 Payroll',\n",
    "    'C185529 LUMINYS': 'Income: Company1 Payroll',\n",
    "    'Money Network DES:': 'Income: Company1 Payroll',\n",
    "    'LUMINYS SYSTEMS DES:PAYMENT': 'Income: Company1 Freelance',\n",
    "    'ISSI INC. DES:PAYROLL': 'Income: Company2 Payroll',\n",
    "    'CERTIFY- LUMINYS': 'Income: Reimbursement',\n",
    "    'Zelle payment to QU WU': 'Income: Reimbursement',\n",
    "    'Interest Earned': 'Income: Credit & Interest',\n",
    "    'CASH REWARDS STATEMENT CREDIT': 'Income: Credit & Interest',\n",
    "    'CASHREWARD': 'Income: Credit & Interest',\n",
    "    'CASHBACK BONUS REDEMPTION': 'Income: Credit & Interest',\n",
    "    \n",
    "    'IRS': 'Tax: Tax Dept.',\n",
    "    'FRANCHISE TAX BD DES': 'Tax: Tax Dept.',\n",
    "    \"Zelle payment to ROGER'S TAX SERVICES LLC\": 'Tax: Roger Service',\n",
    "\n",
    "    'Zelle payment from CHENWEI XU': 'Internal: from User2 to User1',\n",
    "    'Zelle payment to CHENWEI XU': 'Internal: from User1 to User2',\n",
    "    'Zelle payment from NINGCHUAN PENG': 'Internal: from User1 to User2',\n",
    "    'Zelle payment to NINGCHUAN PENG': 'Internal: from User2 to User1',\n",
    "    'Online Banking transfer from SAV 7913': 'Internal: from SA to DC',\n",
    "    'Online Banking transfer to SAV 7913': 'Internal: from DC to SA',\n",
    "    'Online Banking transfer from CHK 9084': 'Internal: from DC to SA',\n",
    "    'Online Banking transfer to CHK 9084': 'Internal: from SA to DC',\n",
    "    'Online Banking transfer from CHK 8540': 'Internal: from DC to SA',\n",
    "    'Online Banking transfer to CHK 8540': 'Internal: from SA to DC',\n",
    "\n",
    "    'Zelle payment to YUKAI GAO': 'Rent: CTHD',\n",
    "    'Zelle payment from YUKAI GAO': 'Rent: CTHD',\n",
    "    'Zelle payment to XUE SHIMING': 'Rent: AVL',\n",
    "    'Zelle payment to QINGMING ZENG': 'Rent: LFV',\n",
    "    'Zelle payment from GUOYUAN WU': 'Rent: LFV',\n",
    "\n",
    "    'AMC': 'Entertainment: AMC',\n",
    "    'LA ARBORETUM': 'Entertainment: Arboretum',\n",
    "    'BELLA BABY PHOTOGRAPHY': 'Entertainment: Baby',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec285b-e087-41de-bd00-16e6be6362fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the description to the type\n",
    "for keyword, mapped_value in description_map.items():\n",
    "    combined_df.loc[combined_df[\"Description\"].str.contains(keyword, case=False, regex=False, na=False), \"Type\"] = mapped_value\n",
    "\n",
    "combined_df.loc[combined_df['Type'].isna(), 'Type'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eda1e8-da73-4761-9a90-0003c83ccf80",
   "metadata": {},
   "source": [
    "# Transaction Offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e7721a-3ba1-4289-8188-17fcf74f6c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary rows and columns\n",
    "for remove_item in combined_df['Type'].unique():\n",
    "    if round(combined_df[combined_df['Type'] == remove_item]['Amount'].sum(), 2) == 0 and remove_item != None:\n",
    "        print('remove: ', remove_item)\n",
    "        combined_df = combined_df[combined_df['Type'] != remove_item]\n",
    "        \n",
    "combined_df = combined_df[combined_df['Amount'] != 0][['Date', 'Id', 'Description', 'Amount', 'Card', 'User', 'Type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2b6a0-a88f-4029-bd86-cd7deb37fe93",
   "metadata": {},
   "source": [
    "# Mapping Old File Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e54e0-ada4-4133-b1c8-ef9f0b7ce90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defin the function of mapping the old excel file information to this new old\n",
    "def sub_old_data(new_df, old_file_name, folder_path):\n",
    "    old_df = pd.read_excel(os.path.join(folder_path, old_file_name), sheet_name = 'Sheet1')[['Id', 'Type', 'General_Type']]\n",
    "\n",
    "    comb_df = pd.merge(new_df, old_df, on = \"Id\", how = \"left\", suffixes = (\"\", \"_old\"))\n",
    "    \n",
    "    comb_df.loc[comb_df['Type'].isin([None, 'Other: Mobile Check']), 'Type'] = comb_df.loc[comb_df['Type'].isin([None, 'Other: Mobile Check']), 'Type_old']\n",
    "\n",
    "    comb_df.loc[comb_df['Type_old'].isin(['Omit: others']), 'Type'] = 'Omit: others'\n",
    "    \n",
    "    return comb_df[['Date', 'Id', 'Description', 'Amount', 'Card', 'User', 'Type', 'General_Type']]\n",
    "\n",
    "# map the old excel file information to this new old\n",
    "old_file_name = 'combined_20250908143823.xlsx'\n",
    "combined_df_new = sub_old_data(combined_df, old_file_name, folder_path)\n",
    "\n",
    "# clean the columns of the dataset and add new columns\n",
    "combined_df_new[\"General_Type\"] = combined_df_new[\"Type\"].str.split(\":\", n = 1).str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ddbf6-5732-40cf-8eb8-3dc343c62a14",
   "metadata": {},
   "source": [
    "# Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d751b34-4722-43b6-b166-e70a082f0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the file to EXCEL\n",
    "excel_name = f\"combined_{datetime.now().strftime('%Y%m%d%H%M%S')}.xlsx\"\n",
    "\n",
    "combined_df_new.to_excel(os.path.join(folder_path, excel_name), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8b555d-4dc1-4040-9b8f-e0af496c491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the google_sheet_id, sheet_name and gsheet_credentials\n",
    "dotenv.load_dotenv(\"../personal_envs/household-cashflow-analyzer/.env\", override = True)\n",
    "spreadsheet_id, worksheet_name = os.getenv(\"gsheet_id\"), os.getenv(\"sheet_name\")\n",
    "gsheet_credentials = \"../personal_envs/household-cashflow-analyzer/gsheet_credentials.json\"\n",
    "\n",
    "# clean the Date column for uploading\n",
    "combined_df_new[\"Date\"] = pd.to_datetime(combined_df_new[\"Date\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# upload the data to Google Sheet\n",
    "gsheet_upload(gsheet_credentials, spreadsheet_id, worksheet_name, combined_df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f199c1-c9e8-409b-ac44-e54745aa6e07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
